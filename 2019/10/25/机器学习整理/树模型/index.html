<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8" />

    

    

    <title>树模型 | 小于狙击手</title>
    <meta name="author" content="于印霄" />
    <meta name="keywords" content="null" />
    <meta name="description" content="树模型总结决策树ID3,C4.5,CART树特征枚举：根据规则选择特征划分节点：根据特征进一步分裂叶子节点根据样本训练出GBDT树，对于每个叶子节点，回溯到根节点都可以得到一组组合特征，所以用叶子节点的标号可以代表一个新的组合特征ID3算法——信息增益度量纯度D为样本集合；$D_v$是特征f取v时的样本集。Gain信息增益=信息熵-条件熵信息熵：H(D)=-\sum_{y=k}^{K}p_k·log(p_k)条件熵：H(D|f)=-\sum_{v\epsilon V_f}\frac..." />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no" />

    
    <link rel="alternate" href="/atom.xml" title="小于狙击手" type="application/atom+xml">
    
    
    <link rel="icon" href="/images/favicon.ico">
    

    <style type="text/css">
    @font-face {
        font-family: 'icomoon';
        src: url("/fonts/icomoon.eot?q628ml");
        src: url("/fonts/icomoon.eot?q628ml#iefix") format('embedded-opentype'),
             url("/fonts/icomoon.ttf?q628ml") format('truetype'),
             url("/fonts/icomoon.woff?q628ml") format('woff'),
             url("/fonts/icomoon.svg?q628ml#icomoon") format('svg');
        font-weight: normal;
        font-style: normal;
    }
    </style>
    <link rel="stylesheet" href="/css/style.css">

    <!--[if lt IE 9]>
    <style type="text/css">
    .nav-inner {top:0;}
    .author-meta {position:static;top:0;}
    .search-form {height:36px;}
    </style>
    <script type="text/javascript" src="https://unpkg.com/html5shiv@3.7.3/dist/html5shiv.min.js"></script>
    <![endif]--><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>
<body>

    <!-- hexo-inject:begin --><!-- hexo-inject:end --><main class="app">
        <header id="header" class="header clearfix">
    <div id="nav" class="nav">
    <div class="nav-mobile">
        <button id="open-panel" class="open-panel nav-mobile-item"><i class="icon-documents"></i></button>
        <h1 class="nav-mobile-title nav-mobile-item">小于狙击手</h1>
        <button id="open-menus" class="open-panel nav-mobile-item"><i class="icon-library"></i></button>
    </div>

    <nav id="nav-inner" class="nav-inner">
        
            <a class="nav-item" href="/">
                <span class="nav-text">首页</span>
            </a>
        
            <a class="nav-item" href="/categories/ml">
                <span class="nav-text">机器学习</span>
            </a>
        
            <a class="nav-item" href="/categories/rec">
                <span class="nav-text">推荐系统</span>
            </a>
        
            <a class="nav-item" href="/categories/ad">
                <span class="nav-text">计算广告</span>
            </a>
        
            <a class="nav-item" href="/categories/BD">
                <span class="nav-text">大数据</span>
            </a>
        
            <a class="nav-item" href="/tags">
                <span class="nav-text">标签</span>
            </a>
        
            <a class="nav-item" href="/archives">
                <span class="nav-text">归档</span>
            </a>
        
            <a class="nav-item" href="/about">
                <span class="nav-text">关于我</span>
            </a>
        
    </nav>
</div>

    <aside id="aside" class="aside">
    <div id="aside-mask" class="aside-mask"></div>
    <div id="aside-inner" class="aside-inner">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit"><i class="icon-search-stroke"></i></button><input type="hidden" name="sitesearch" value="http://yuyinxiao.github.io"></form>

        
        
        
        

        
        <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#树模型总结"><span class="toc-number">1.</span> <span class="toc-text">树模型总结</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#决策树ID3-C4-5-CART树"><span class="toc-number">1.1.</span> <span class="toc-text">决策树ID3,C4.5,CART树</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#ID3算法——信息增益度量纯度"><span class="toc-number">1.1.1.</span> <span class="toc-text">ID3算法——信息增益度量纯度</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#C4-5算法——信息增益比度量纯度"><span class="toc-number">1.1.2.</span> <span class="toc-text">C4.5算法——信息增益比度量纯度</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#CART树——Gini系数度量纯度"><span class="toc-number">1.1.3.</span> <span class="toc-text">CART树——Gini系数度量纯度</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#CART树，ID3，C4-5对比"><span class="toc-number">1.1.3.1.</span> <span class="toc-text">CART树，ID3，C4.5对比</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#集成学习"><span class="toc-number">1.2.</span> <span class="toc-text">集成学习</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Random-Forest"><span class="toc-number">1.2.1.</span> <span class="toc-text">Random Forest</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#GBDT"><span class="toc-number">1.2.2.</span> <span class="toc-text">GBDT</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#GBDT多分类以及训练过程"><span class="toc-number">1.2.2.1.</span> <span class="toc-text">GBDT多分类以及训练过程</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#求解树结构复杂度："><span class="toc-number">1.2.2.2.</span> <span class="toc-text">求解树结构复杂度：</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#XGBoost"><span class="toc-number">1.2.3.</span> <span class="toc-text">XGBoost</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#GBDT-vs-XGB"><span class="toc-number">1.2.3.1.</span> <span class="toc-text">GBDT vs XGB</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#FM算法"><span class="toc-number">1.3.</span> <span class="toc-text">FM算法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#FM的计算"><span class="toc-number">1.3.1.</span> <span class="toc-text">FM的计算</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#计算过程"><span class="toc-number">1.3.2.</span> <span class="toc-text">计算过程</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#GBDT-FM"><span class="toc-number">1.4.</span> <span class="toc-text">GBDT+FM</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#稀疏特征进FM"><span class="toc-number">1.4.1.</span> <span class="toc-text">稀疏特征进FM</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#连续特征进GBDT"><span class="toc-number">1.4.2.</span> <span class="toc-text">连续特征进GBDT</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#DNN的应用"><span class="toc-number">1.4.3.</span> <span class="toc-text">DNN的应用</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#GBDT-FM为什么不是RF-FM"><span class="toc-number">1.4.4.</span> <span class="toc-text">GBDT+FM为什么不是RF+FM</span></a></li></ol></li></ol></li></ol>
        
    </div>
</aside>

</header>

        <div id="content" class="content">
            <div id="wrapper" class="wrapper" style="max-width: 800px">
                <article class="article" itemscope itemprop="blogPost">
    
    <header class="article-header">
        
        <h1 itemprop="name">
            树模型
        </h1>
        
        <div class="article-meta clearfix">
            <a class="article-date" href="http://yuyinxiao.github.io/2019/10/25/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%95%B4%E7%90%86/%E6%A0%91%E6%A8%A1%E5%9E%8B/index.html">
    
    <i class="icon-calendar"></i>
    
    <time datetime="2019-10-25T05:37:00.000Z" itemprop="datePublished">2019-10-25</time>
</a>

            
<div class="article-tag-list">
    <i class="icon-tag"></i>
    <a class="article-tag-link" href="/tags/ML/" rel="tag">ML</a>
</div>


        </div>
    </header>
    
    <section class="article-body markdown-body">
        
        <h1 id="树模型总结"><a href="#树模型总结" class="headerlink" title="树模型总结"></a>树模型总结</h1><h2 id="决策树ID3-C4-5-CART树"><a href="#决策树ID3-C4-5-CART树" class="headerlink" title="决策树ID3,C4.5,CART树"></a>决策树ID3,C4.5,CART树</h2><p><img src="/2019/10/25/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%95%B4%E7%90%86/%E6%A0%91%E6%A8%A1%E5%9E%8B/1.png" alt="avatar"><br>特征枚举：根据规则选择特征<br>划分节点：根据特征进一步分裂叶子节点<br><strong>根据样本训练出GBDT树，对于每个叶子节点，回溯到根节点都可以得到一组组合特征，所以用叶子节点的标号可以代表一个新的组合特征</strong><br><a id="more"></a></p>
<h3 id="ID3算法——信息增益度量纯度"><a href="#ID3算法——信息增益度量纯度" class="headerlink" title="ID3算法——信息增益度量纯度"></a>ID3算法——信息增益度量纯度</h3><p>D为样本集合；$D_v$是特征f取v时的样本集。Gain信息增益=信息熵-条件熵</p>
<ul>
<li>信息熵：<script type="math/tex; mode=display">H(D)=-\sum_{y=k}^{K}p_k·log(p_k)</script></li>
<li>条件熵：<script type="math/tex; mode=display">H(D|f)=-\sum_{v\epsilon V_f}\frac{|D_v|}{|D|}H(D_v)</script></li>
<li>信息增益：<script type="math/tex; mode=display">Gain(D,f)=H(D)-H(D|f)</script></li>
<li>缺点：<strong>信息增益准则倾向于选取多值特征！</strong>假如把“编号”也作为一个候选划分属性，因为每一个样本的编号都是不同的（由于编号独特唯一，条件熵为0，每一个结点中只有一类，纯度非常高），也就是说，来了一个预测样本，你只要告诉我编号，其它特征就没有用了，这样生成的决策树显然不具有泛化能力。</li>
</ul>
<h3 id="C4-5算法——信息增益比度量纯度"><a href="#C4-5算法——信息增益比度量纯度" class="headerlink" title="C4.5算法——信息增益比度量纯度"></a>C4.5算法——信息增益比度量纯度</h3><p>信息增益比：</p>
<script type="math/tex; mode=display">Gain\_ratio(D,f)=\frac{Gain(D,f)}{IV(f)}</script><script type="math/tex; mode=display">IV(f)=-\sum_{y=k}^{K}p_{k,f}·log(p_{k,f})</script><p>IV(f)表明：某属性分成的K类别数越大，IV(f)就越大；是对多值属性的罚项，描述该特征下类别的离散程度。C4.5算法不直接选择增益率最大的候选划分属性，候选划分属性中找出信息增益高于平均水平的属性（这样保证了大部分好的的特征），再从中选择增益率最高的（又保证了不会出现编号特征这种极端的情况）</p>
<ul>
<li>优点<ol>
<li>可处理连续特征，排序取中间值。</li>
</ol>
</li>
</ul>
<h3 id="CART树——Gini系数度量纯度"><a href="#CART树——Gini系数度量纯度" class="headerlink" title="CART树——Gini系数度量纯度"></a>CART树——Gini系数度量纯度</h3><ul>
<li>总体样本的Gini系数：<script type="math/tex; mode=display">Gini(D)=1-\sum_{y=k}^{K}(\frac{D_k}{D})^2</script></li>
<li>特征f下的Gini系数<script type="math/tex; mode=display">Gini(D|f)=1-\sum_{v\epsilon V_f}\frac{D_v}{D}Gini(D_v)</script><strong>无论是回归还是分类问题，无论特征是离散的还是连续的，无论特征取值有多个还是两个，CART树内部节点只能根据属性值进行二分</strong></li>
<li>作为回归树：使用最小化MSE来选择特征并进行划分。遍历特征以及取值尝试划分，每一个叶子节点给出的预测值，是该叶子节点中所有样本的均值，计算最小平方误差。回归树生成使用平方误差最小化准则。</li>
<li>作为分类树：使用Gini指数最小化准则来选择特征并进行划分。Gini指数表示集合的不确定性，或者是不纯度。基尼指数越大，集合不确定性越高，不纯度也越大。这一点和熵类似。另一种理解基尼指数的思路是，基尼指数是为了最小化误分类的概率。</li>
</ul>
<h4 id="CART树，ID3，C4-5对比"><a href="#CART树，ID3，C4-5对比" class="headerlink" title="CART树，ID3，C4.5对比"></a>CART树，ID3，C4.5对比</h4><div class="table-container">
<table>
<thead>
<tr>
<th>算法</th>
<th>树</th>
<th>准则</th>
<th>多分类</th>
<th>回归</th>
<th>特征</th>
<th>缺失值</th>
<th>特点</th>
</tr>
</thead>
<tbody>
<tr>
<td>ID3</td>
<td>多叉树</td>
<td>信息增益</td>
<td>仅二分类</td>
<td>不能</td>
<td>仅离散</td>
<td>敏感</td>
<td>倾向多值特征    </td>
</tr>
<tr>
<td>C4.5</td>
<td>多叉树</td>
<td>信息增益比</td>
<td>仅二分类</td>
<td>不能</td>
<td>离散/连续</td>
<td>可处理</td>
<td>偏向少值特征。实际信息增益+信息增益比    </td>
</tr>
<tr>
<td>CART</td>
<td>仅二叉树</td>
<td>Gini系数</td>
<td>多分类</td>
<td>能</td>
<td>离散/连续</td>
<td>可处理</td>
<td>不需对数运算，更偏向于连续属性    </td>
</tr>
</tbody>
</table>
</div>
<h2 id="集成学习"><a href="#集成学习" class="headerlink" title="集成学习"></a>集成学习</h2><p>集成学习——bagging与boosting<br><img src="/2019/10/25/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%95%B4%E7%90%86/%E6%A0%91%E6%A8%A1%E5%9E%8B/4.png" alt="avatar"><br><img src="/2019/10/25/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%95%B4%E7%90%86/%E6%A0%91%E6%A8%A1%E5%9E%8B/3.png" alt="avatar"></p>
<div class="table-container">
<table>
<thead>
<tr>
<th>集成学习</th>
<th>基学习器</th>
<th>依赖关系</th>
<th>串/并行</th>
<th>复杂度</th>
<th>关注点</th>
<th>应用</th>
<th>预测结果</th>
<th>数据使用</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
</tr>
</tbody>
</table>
</div>
<p>bagging|强|弱|并行|基学习器同阶|降方差|可回归，可分类|投票/平均|有放回抽取<br>boosting|弱|强|串行|基学习器叠加|降偏差|可回归，可分类|累加/阈值|原数据样本权重会改变    </p>
<h3 id="Random-Forest"><a href="#Random-Forest" class="headerlink" title="Random Forest"></a>Random Forest</h3><p>多棵决策树，随机样本，随即特征进行划分。<br>分类：投票；回归：平均。<br>样本随机：63.2%（1-1/e）数据会被选区，<strong>36.8%(1/e)未被用到称为OOB样本，包外估计</strong>。<br>特征随机：从该节点特征集随机抽取K个，然后选择最优属性。<br>容易做成并行化方法</p>
<h3 id="GBDT"><a href="#GBDT" class="headerlink" title="GBDT"></a>GBDT</h3><p><img src="/2019/10/25/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%95%B4%E7%90%86/%E6%A0%91%E6%A8%A1%E5%9E%8B/9.png" alt="avatar"></p>
<ol>
<li>初始化第一棵树；</li>
<li>计算损失对前面k-1棵树的负梯度；</li>
<li>训练拟合得到第k棵树；</li>
<li>最小化损失获得步长η；将新的树添加到模型Fk</li>
</ol>
<ul>
<li>函数空间的梯度下降，函数拟合的是负梯度；最终函数等于每次迭代的增量的累加和。</li>
<li>参数空间的梯度下降，参数更新方向为负梯度方向；最终参数等于每次迭代的增量累加和。<br>新的一棵树拟合的是模型的整体损失对前面k-1棵树的负梯度。</li>
</ul>
<h4 id="GBDT多分类以及训练过程"><a href="#GBDT多分类以及训练过程" class="headerlink" title="GBDT多分类以及训练过程"></a>GBDT多分类以及训练过程</h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">X=[];label=[<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>]</span><br><span class="line">(X,<span class="number">0</span>) -&gt; t1;得到预测值f1(X)</span><br><span class="line">(X,<span class="number">1</span>) -&gt; t2;得到预测值f2(X)</span><br><span class="line">(X,<span class="number">0</span>) -&gt; t3;得到预测值f3(X)</span><br><span class="line">对f1(x),f2(x),f3(x)进行softmax产生概率。</span><br><span class="line">得到残差：</span><br><span class="line">类别<span class="number">1</span>：y11(x)=<span class="number">0</span>−p1(x);</span><br><span class="line">类别<span class="number">2</span>：y22(x)=<span class="number">1</span>−p2(x);</span><br><span class="line">类别<span class="number">3</span>：y33(x)=<span class="number">0</span>−p3(x);</span><br><span class="line">-------------------------------</span><br><span class="line">(X,y11) -&gt; t11;得到预测值f11(X)</span><br><span class="line">(X,y22) -&gt; t22;得到预测值f22(X)</span><br><span class="line">(X,y33) -&gt; t33;得到预测值f33(X)</span><br><span class="line">……</span><br></pre></td></tr></table></figure>
<h4 id="求解树结构复杂度："><a href="#求解树结构复杂度：" class="headerlink" title="求解树结构复杂度："></a>求解树结构复杂度：</h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">树深度K:</span><br><span class="line">    枚举所有特征d:</span><br><span class="line">        该特征下值排序 nlogn</span><br><span class="line">        线性扫描决定最佳分裂点</span><br><span class="line">        计算分裂后MSE或者Gini系数选择最优分裂点</span><br></pre></td></tr></table></figure>
<p>总体复杂度：<strong>K <em> d </em> nlogn。</strong></p>
<h3 id="XGBoost"><a href="#XGBoost" class="headerlink" title="XGBoost"></a>XGBoost</h3><p>xgb与gbdt的算法对比<br><img src="/2019/10/25/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%95%B4%E7%90%86/%E6%A0%91%E6%A8%A1%E5%9E%8B/7.png" alt="avatar"><br><img src="/2019/10/25/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%95%B4%E7%90%86/%E6%A0%91%E6%A8%A1%E5%9E%8B/11.jpg" alt="avatar"><br><img src="/2019/10/25/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%95%B4%E7%90%86/%E6%A0%91%E6%A8%A1%E5%9E%8B/5.png" alt="avatar"></p>
<ul>
<li><p>GBDT：<br>GBDT构造新的一棵树，对损失近似到一阶导，拟合负梯度；属于函数空间中利用梯度下降法进行优化</p>
</li>
<li><p>XGBoost：<br>XGBoost构造新的一棵树，对损失近似到二阶导，拟合负一阶导/二阶导(+正则项)；属于函数空间中的牛顿法进行优化</p>
</li>
</ul>
<h4 id="GBDT-vs-XGB"><a href="#GBDT-vs-XGB" class="headerlink" title="GBDT vs XGB"></a>GBDT vs XGB</h4><div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th>GBDT</th>
<th>XGB</th>
</tr>
</thead>
<tbody>
<tr>
<td>基分类器</td>
<td>树模型</td>
<td>树模型+LR回归</td>
</tr>
</tbody>
</table>
</div>
<p>树拟合|负梯度|负梯度/二阶导<br>方差-偏差权衡||目标函数+正则（叶子节点数，节点权重）<br>分割准则|回归：MSE；分类：基尼系数|最大化损失增益<br>分割方法||分位数法列举候选分割，求Gain最佳<br>缺失值处理||指定学习缺失值分类方向；忽略缺失值<br>|||列抽样，数据集抽样<br>|||并行化：特征值排序找出候选点，保存为列块结构，各个特征增益计算并行运算<br><a href="https://mp.weixin.qq.com/s/a4v9n_hUgxNyKSQ3RgDMLA" target="_blank" rel="noopener">XGB面试精选</a></p>
<h2 id="FM算法"><a href="#FM算法" class="headerlink" title="FM算法"></a>FM算法</h2><ul>
<li>优点： 减少特征交叉的选择工作，特征参数学习更加充分，提升参数学习效率和模型预估能力。</li>
<li>缺点：两两交叉，相比于DNN，GBDT更多特征的交叉</li>
</ul>
<h3 id="FM的计算"><a href="#FM的计算" class="headerlink" title="FM的计算"></a>FM的计算</h3><p><img src="/2019/10/25/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%95%B4%E7%90%86/%E6%A0%91%E6%A8%A1%E5%9E%8B/fm.png" alt><br><img src="https://upload-images.jianshu.io/upload_images/4155986-6d08a2cdcc6668fb.png?imageMogr2/auto-orient/strip|imageView2/2/w/413/format/webp" alt></p>
<h3 id="计算过程"><a href="#计算过程" class="headerlink" title="计算过程"></a>计算过程</h3><p><img src="https://images2018.cnblogs.com/blog/1473228/201809/1473228-20180907130132713-283779506.png" alt="avatar"><br><img src="https://images2018.cnblogs.com/blog/1473228/201809/1473228-20180907114648527-2084814077.png" alt="avatar"><br><img src="https://images2018.cnblogs.com/blog/1473228/201809/1473228-20180907124807939-1507262905.png" alt="avatar"></p>
<h2 id="GBDT-FM"><a href="#GBDT-FM" class="headerlink" title="GBDT+FM"></a>GBDT+FM</h2><p>GBDT一棵树的叶子节点one-hot构成组合特征，该颗树特征对应一个K维向量作为该特$v_i$，进FM时两棵树的组合就是$v_i$和$v_j$的点成得到$w_{ij}$实际上是更多低阶特征的组合离散特征学习低维dense特征；</p>
<hr>
<ol>
<li>提取统计类特征使用gbdt模型快速拿到收益;</li>
<li>加入海量离散类特征（个性化），使用LR/FM模型进一步提升效果。</li>
<li>至于原有的统计类特征可以通过gbdt叶子节点转换成离散特征一并加入到LR/FM中。<h3 id="稀疏特征进FM"><a href="#稀疏特征进FM" class="headerlink" title="稀疏特征进FM"></a>稀疏特征进FM</h3><strong>高维稀疏特征适合线性模型；LR+L2正则可以使得权重降低，不容易对稀疏特征过拟合。</strong><h3 id="连续特征进GBDT"><a href="#连续特征进GBDT" class="headerlink" title="连续特征进GBDT"></a>连续特征进GBDT</h3><strong>统计特征，低维稠密特征适合树模型</strong>；一般统计特征具有总体的特征效果或全局性的因素</li>
</ol>
<blockquote>
<blockquote>
<p>离散特征进GBDT时根据0，1划分节点偶然性太大，很容易过拟合；特征太多，树模型不易训练</p>
</blockquote>
</blockquote>
<h3 id="DNN的应用"><a href="#DNN的应用" class="headerlink" title="DNN的应用"></a>DNN的应用</h3><p>解决FM特征交叉能力受限的问题。<strong>DNN较容易学习到高阶非线性特征</strong></p>
<h3 id="GBDT-FM为什么不是RF-FM"><a href="#GBDT-FM为什么不是RF-FM" class="headerlink" title="GBDT+FM为什么不是RF+FM"></a>GBDT+FM为什么不是RF+FM</h3><ul>
<li>GBDT前面的树侧重对多数样本有区分度的特征；后面的树针对经过前面的树仍然残差大的少数样本</li>
<li>从bias and variance角度，RF降低variance而不改变bias, 而LR线性模型的variance足够低，需要更多的组合特征降低bias</li>
</ul>

        
    </section>
</article>



<a id="pagenext" href="/2019/10/25/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%95%B4%E7%90%86/%E7%BB%86%E8%8A%82%E8%A1%A5%E5%85%85/" class="article-next" title="Faiss"><i class="icon-arrow-right"></i></a>


<a id="pageprev" href="/2019/10/25/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%95%B4%E7%90%86/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/" class="article-prev" title="机器学习基础"><i class="icon-arrow-left"></i></a>



<div class="comments">
    <div id="comments"></div>
    <script src="https://unpkg.com/gitalk/dist/gitalk.min.js"></script>
    <script>
    new Gitalk({
        clientID: "a9a645b881c78d12baa8",
        clientSecret: "d7e2f110f4885b3fa11894f3ddd2cd46523889fd",
        repo: "yuyinxiao.github.io",
        owner: "yuyinxiao",
        admin: ["yuyinxiao"],
        id: "2019/10/25/机器学习整理/树模型",
        distractionFreeMode: true,
        title: "树模型",
        body: "http://yuyinxiao.github.io/2019/10/25/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%95%B4%E7%90%86/%E6%A0%91%E6%A8%A1%E5%9E%8B/",
        labels: ["ML"]
    }).render('comments');
    </script>
</div>


            </div>
        </div>
        <footer class="footer">
    Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>, Theme by <a href="https://github.com/sanonz/hexo-theme-concise" target="_blank">Concise</a>

    
    <script>
        var _hmt = _hmt || [];
        (function () {
            var hm = document.createElement("script");
            hm.src = "https://hm.baidu.com/hm.js?ff62cebe12c54c12ef10722f77d75303";
            var s = document.getElementsByTagName("script")[0];
            s.parentNode.insertBefore(hm, s);
        })();
    </script>
    
</footer>

    </main>

    <script type="text/javascript" src="https://cdn.bootcss.com/jquery/1.9.0/jquery.min.js"></script>
    <script type="text/javascript">
    $(function() {
        var nodes = {
            nav: $('#nav'),
            aside: $('#aside'),
            asideInner: $('#aside-inner'),
            navInner: $('#nav-inner')
        };

        var doing = false;
        nodes.asideInner.on('webkitAnimationEnd mozAnimationEnd oAnimationEnd oanimationend animationend', function() {
            if (nodes.aside.hasClass('mobile-open')) {
                nodes.aside.removeClass('mobile-open');
            } else {
                nodes.aside.removeClass('mobile-close panel-show');
            }
            doing = false;
        });
        $('#open-panel, #aside-mask').on('click', function() {
            if (doing) {
                return;
            }

            if (nodes.aside.hasClass('panel-show')) {
                nodes.aside.addClass('mobile-close');
            } else {
                nodes.aside.addClass('mobile-open panel-show');
            }
        });
        $('#open-menus').on('click', function() {
            nodes.navInner.slideToggle();
        });

        if (window.innerWidth <= 960) {
            setTimeout(function() {
                nodes.navInner.slideUp();
            }, 3000);
        }
    });
    </script>
    
        <script src="/js/scrollspy.min.js"></script>
        <script type="text/javascript">
        $(document.body).scrollspy({target: '#aside-inner'});

        $(window).on('resize', function() {
            var hw = $('#header').width();
            var ww = $('#wrapper').width();
            var space = ($(this).width() - hw - ww) / 2 / 2;

            var pageprev = $('#pageprev');
            var pagenext = $('#pagenext');
            var avg = (pageprev.width() + pagenext.width()) / 2

            if(space > avg) {
                var len = space - avg / 2;
                var styles = {position: 'fixed', top: '50%', marginTop: - (pageprev.width() + pagenext.width()) / 4}
                pageprev.css($.extend({left: hw + len}, styles));
                pagenext.css($.extend({right: len}, styles));
            } else {
                pageprev.removeAttr('style');
                pagenext.removeAttr('style');
            }
        }).trigger('resize');
        </script>
    

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</body>
</html>
