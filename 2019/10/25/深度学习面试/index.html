<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8" />

    

    

    <title>深度学习面试 | 小于狙击手的博客</title>
    <meta name="author" content="于印霄" />
    <meta name="keywords" content="null" />
    <meta name="description" content="深度学习激活函数如何使用激活函数回归，则将sigmoid或tanh用作最后一层的非线性和平方误差作为成本函数。分类，隐层relu+尾层sigmoid+softmax归一化输出+结果交叉熵作损失为什么非线性激活函数神经网络万能近似定理认为神经网络具有至少一个非线性隐藏层，那么只要给予网络足够数量的隐藏单元，隐层就可以以任意的精度来拟合函数：完成任何一个有限维空间到另一个有限维空间的函数。如果不使用非线性激活函数，那么每一层输出都是上层输入的线性组合，整体也是线性的，失去万能近似的..." />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no" />

    
    <link rel="alternate" href="/atom.xml" title="小于狙击手的博客" type="application/atom+xml">
    
    
    <link rel="icon" href="/images/favicon.ico">
    

    <style type="text/css">
    @font-face {
        font-family: 'icomoon';
        src: url("/fonts/icomoon.eot?q628ml");
        src: url("/fonts/icomoon.eot?q628ml#iefix") format('embedded-opentype'),
             url("/fonts/icomoon.ttf?q628ml") format('truetype'),
             url("/fonts/icomoon.woff?q628ml") format('woff'),
             url("/fonts/icomoon.svg?q628ml#icomoon") format('svg');
        font-weight: normal;
        font-style: normal;
    }
    </style>
    <link rel="stylesheet" href="/css/style.css">

    <!--[if lt IE 9]>
    <style type="text/css">
    .nav-inner {top:0;}
    .author-meta {position:static;top:0;}
    .search-form {height:36px;}
    </style>
    <script type="text/javascript" src="https://unpkg.com/html5shiv@3.7.3/dist/html5shiv.min.js"></script>
    <![endif]--><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>
<body>

    <!-- hexo-inject:begin --><!-- hexo-inject:end --><main class="app">
        <header id="header" class="header clearfix">
    <div id="nav" class="nav">
    <div class="nav-mobile">
        <button id="open-panel" class="open-panel nav-mobile-item"><i class="icon-documents"></i></button>
        <h1 class="nav-mobile-title nav-mobile-item">小于狙击手的博客</h1>
        <button id="open-menus" class="open-panel nav-mobile-item"><i class="icon-library"></i></button>
    </div>

    <nav id="nav-inner" class="nav-inner">
        
            <a class="nav-item" href="/categories/home">
                <span class="nav-text">首页</span>
            </a>
        
            <a class="nav-item" href="/categories/ml">
                <span class="nav-text">机器学习</span>
            </a>
        
            <a class="nav-item" href="/categories/rec">
                <span class="nav-text">推荐系统</span>
            </a>
        
            <a class="nav-item" href="/categories/BD">
                <span class="nav-text">大数据</span>
            </a>
        
            <a class="nav-item" href="/categories/code">
                <span class="nav-text">coding</span>
            </a>
        
            <a class="nav-item" href="/tags">
                <span class="nav-text">标签</span>
            </a>
        
            <a class="nav-item" href="/archives">
                <span class="nav-text">归档</span>
            </a>
        
            <a class="nav-item" href="/about">
                <span class="nav-text">关于我</span>
            </a>
        
    </nav>
</div>

    <aside id="aside" class="aside">
    <div id="aside-mask" class="aside-mask"></div>
    <div id="aside-inner" class="aside-inner">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit"><i class="icon-search-stroke"></i></button><input type="hidden" name="sitesearch" value="https://yuyinxiao.github.io"></form>

        
        
        
        

        
        <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#深度学习"><span class="toc-number">1.</span> <span class="toc-text">深度学习</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#激活函数"><span class="toc-number">1.1.</span> <span class="toc-text">激活函数</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#如何使用激活函数"><span class="toc-number">1.1.1.</span> <span class="toc-text">如何使用激活函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#为什么非线性激活函数"><span class="toc-number">1.1.2.</span> <span class="toc-text">为什么非线性激活函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#各种激活函数优缺点"><span class="toc-number">1.1.3.</span> <span class="toc-text">各种激活函数优缺点</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#二、正则化"><span class="toc-number">1.2.</span> <span class="toc-text">二、正则化</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1、批归一化"><span class="toc-number">1.2.1.</span> <span class="toc-text">1、批归一化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2、L1和L2正则"><span class="toc-number">1.2.2.</span> <span class="toc-text">2、L1和L2正则</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#L1、L2区别"><span class="toc-number">1.2.2.1.</span> <span class="toc-text">L1、L2区别</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3、Dropout"><span class="toc-number">1.2.3.</span> <span class="toc-text">3、Dropout</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4、参数初始化"><span class="toc-number">1.2.4.</span> <span class="toc-text">4、参数初始化</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#神经网络的偏置项b"><span class="toc-number">1.3.</span> <span class="toc-text">神经网络的偏置项b</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#NN的参数初始化"><span class="toc-number">1.4.</span> <span class="toc-text">NN的参数初始化</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#交叉熵损失与平方差损失"><span class="toc-number">1.5.</span> <span class="toc-text">交叉熵损失与平方差损失</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#神经网络和SVM的区别"><span class="toc-number">1.6.</span> <span class="toc-text">神经网络和SVM的区别</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#神经网络的宽度和深度"><span class="toc-number">1.7.</span> <span class="toc-text">神经网络的宽度和深度</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#神经网络万能近似定理"><span class="toc-number">1.8.</span> <span class="toc-text">神经网络万能近似定理</span></a></li></ol></li></ol>
        
    </div>
</aside>

</header>

        <div id="content" class="content">
            <div id="wrapper" class="wrapper" style="max-width: 800px">
                <article class="article" itemscope itemprop="blogPost">
    
    <header class="article-header">
        
        <h1 itemprop="name">
            深度学习面试
        </h1>
        
        <div class="article-meta clearfix">
            <a class="article-date" href="https://yuyinxiao.github.io/2019/10/25/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E9%9D%A2%E8%AF%95/index.html">
    
    <i class="icon-calendar"></i>
    
    <time datetime="2019-10-25T05:53:01.000Z" itemprop="datePublished">2019-10-25</time>
</a>

            
<div class="article-tag-list">
    <i class="icon-tag"></i>
    <a class="article-tag-link" href="/tags/DL/" rel="tag">DL</a>
</div>


        </div>
    </header>
    
    <section class="article-body markdown-body">
        
        <h1 id="深度学习"><a href="#深度学习" class="headerlink" title="深度学习"></a>深度学习</h1><h2 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h2><h3 id="如何使用激活函数"><a href="#如何使用激活函数" class="headerlink" title="如何使用激活函数"></a>如何使用激活函数</h3><ul>
<li>回归，则将sigmoid或tanh用作最后一层的非线性和平方误差作为成本函数。</li>
<li>分类，隐层relu+尾层sigmoid+softmax归一化输出+结果交叉熵作损失</li>
</ul>
<h3 id="为什么非线性激活函数"><a href="#为什么非线性激活函数" class="headerlink" title="为什么非线性激活函数"></a>为什么非线性激活函数</h3><ul>
<li><strong>神经网络万能近似定理</strong>认为神经网络具有至少一个非线性隐藏层，那么只要给予网络足够数量的隐藏单元，<strong>隐层就可以以任意的精度来拟合函数：完成任何一个有限维空间到另一个有限维空间的函数。</strong></li>
<li>如果不使用非线性激活函数，那么每一层输出都是上层输入的线性组合，整体也是线性的，失去万能近似的性质。</li>
<li>部分层是纯线性是可以接受的，这有助于减少网络中的参数。<a id="more"></a>
<h3 id="各种激活函数优缺点"><a href="#各种激活函数优缺点" class="headerlink" title="各种激活函数优缺点"></a>各种激活函数优缺点</h3></li>
<li>$sigmoid$<ol>
<li>优点：较好的解释性；</li>
<li>缺点：函数饱和梯度消失；非0中心，梯度恒为正或负；指数运算慢；</li>
</ol>
</li>
<li>$tanh$<ol>
<li>优点：0中心；</li>
<li>缺点：饱和梯度消失，计算慢；</li>
</ol>
</li>
<li>$ReLu$<ol>
<li>优点：加速梯度下降收敛；线性非饱和；特别适用DNN；</li>
<li>缺点：负半区失活；稀疏；</li>
</ol>
</li>
</ul>
<h2 id="二、正则化"><a href="#二、正则化" class="headerlink" title="二、正则化"></a>二、正则化</h2><h3 id="1、批归一化"><a href="#1、批归一化" class="headerlink" title="1、批归一化"></a>1、批归一化</h3><ul>
<li>什么是批归一化<ul>
<li>每一批数据层层传播，致使网络在每次迭代中都需要拟合不同的数据分布，增大了网络的训练难度与过拟合的风险。对某层的神经元进行归一化（标准正态）处理，使其限制在统一分布下。相当于<strong>对数据分布进行额外的约束，降低拟合能力，破坏已学到的特征分布，从而增强模型的泛化能力。</strong></li>
</ul>
</li>
<li>批归一化的作用<ol>
<li>加速网络的训练（缓解梯度消失，支持更大的学习率）</li>
<li>防止过拟合</li>
<li>降低了参数初始化的要求。</li>
</ol>
</li>
</ul>
<h3 id="2、L1和L2正则"><a href="#2、L1和L2正则" class="headerlink" title="2、L1和L2正则"></a>2、L1和L2正则</h3><h4 id="L1、L2区别"><a href="#L1、L2区别" class="headerlink" title="L1、L2区别"></a>L1、L2区别</h4><ul>
<li>相同点：通过限制参数的规模，使模型偏好于权值较小的目标函数，防止过拟合。</li>
<li>不同点<ol>
<li>L1 正则化可以产生更稀疏的权值矩阵，可以用于特征选择，同时一定程度上防止过拟合；L1 正则化适用于特征之间有关联的情况。</li>
<li>L2 正则化整体降低模型权重，模型复杂度更低，模型对于噪声与异常点的抗干扰性的能力增强，从而提高模型的泛化能力。</li>
</ol>
</li>
</ul>
<h3 id="3、Dropout"><a href="#3、Dropout" class="headerlink" title="3、Dropout"></a>3、Dropout</h3><blockquote>
<blockquote>
<p>训练时丢失，预测时都加上</p>
</blockquote>
</blockquote>
<p>隐藏层的采样概率为 0.5，输入的采样概率为 0.8；<br><img src="/2019/10/25/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E9%9D%A2%E8%AF%95/bn.png" alt="avatar"></p>
<h3 id="4、参数初始化"><a href="#4、参数初始化" class="headerlink" title="4、参数初始化"></a>4、参数初始化</h3><ol>
<li>一般使用服从的高斯分布（mean=0, stddev=1）或均匀分布的随机值作为权重的初始化参数；使用 0 作为偏置的初始化参数</li>
<li>一些启发式方法会根据输入与输出的单元数来决定初始值的范围；比如<br><img src="/2019/10/25/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E9%9D%A2%E8%AF%95/1.png" alt="avatar"><br>Keras 全连接层默认的权重初始化方法</li>
</ol>
<h2 id="神经网络的偏置项b"><a href="#神经网络的偏置项b" class="headerlink" title="神经网络的偏置项b"></a>神经网络的偏置项b</h2><script type="math/tex; mode=display">\hat y=\sum_i w_i x_i</script><script type="math/tex; mode=display">y=1,if\hat{y}>= threshold</script><script type="math/tex; mode=display">y=0,if\hat{y}<= threshold</script><p>$threshold$度量神经元正负激励的难易程度，$threshold$越大产生正向激励难度越大。通过移项作为$-b$加入最终的值中判断是否大于0即可。所以<strong>$b$的大小，度量了神经元产生正（负）激励的难易程度</strong></p>
<h2 id="NN的参数初始化"><a href="#NN的参数初始化" class="headerlink" title="NN的参数初始化"></a>NN的参数初始化</h2><p>如果无隐层则参数可以都初始化为0；其他不会使用0初始化；</p>
<h2 id="交叉熵损失与平方差损失"><a href="#交叉熵损失与平方差损失" class="headerlink" title="交叉熵损失与平方差损失"></a>交叉熵损失与平方差损失</h2><p>对分类问题</p>
<ul>
<li>交叉熵度量两个分布的相似性；平方差损失度量两个数据距离；</li>
<li>交叉熵对概率更为敏感；平方差对数值敏感；</li>
<li>分类问题最后是sigmoid的输出，交叉熵可以使误差越大时学习率越大收敛快；平方差损失学习则慢；</li>
</ul>
<p>平方损失用在输出连续，最后一层无sigmoid和softmax层的NN中。</p>
<h2 id="神经网络和SVM的区别"><a href="#神经网络和SVM的区别" class="headerlink" title="神经网络和SVM的区别"></a>神经网络和SVM的区别</h2><p>SVM和NN都可实现非线性分类模型。</p>
<ul>
<li>NN通过多个隐层实现非线性函数-<strong>神经网络万能近似定理</strong>。设计灵活，但可解释性较差；</li>
<li>VM使用核函数的方法，实现一个泛函线性空间。核函数设计复杂；可解释性较强，理论完备；</li>
</ul>
<h2 id="神经网络的宽度和深度"><a href="#神经网络的宽度和深度" class="headerlink" title="神经网络的宽度和深度"></a>神经网络的宽度和深度</h2><p>宽度设计：参数过多；<br>深度设计：参数较少。</p>
<h2 id="神经网络万能近似定理"><a href="#神经网络万能近似定理" class="headerlink" title="神经网络万能近似定理"></a>神经网络万能近似定理</h2><p>一个前馈神经网络如果具有线性层和至少一个“挤压”性质的激活函数，给定网络足够的隐层，它可以以任意精度近似一个函数，实现一个有限维空间到另一个有限维空间的映射。</p>

        
    </section>
</article>



<a id="pagenext" href="/2019/10/25/Transformer/" class="article-next" title="Transformer"><i class="icon-arrow-right"></i></a>


<a id="pageprev" href="/2019/10/25/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/" class="article-prev" title="优化算法"><i class="icon-arrow-left"></i></a>



<div class="comments">
    <div id="comments"></div>
    <script src="https://unpkg.com/gitalk/dist/gitalk.min.js"></script>
    <script>
    new Gitalk({
        clientID: "a9a645b881c78d12baa8",
        clientSecret: "d7e2f110f4885b3fa11894f3ddd2cd46523889fd",
        repo: "yuyinxiao.github.io",
        owner: "yuyinxiao",
        admin: ["yuyinxiao"],
        id: "2019/10/25/深度学习面试",
        distractionFreeMode: true,
        title: "深度学习面试",
        body: "https://yuyinxiao.github.io/2019/10/25/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E9%9D%A2%E8%AF%95/",
        labels: ["DL"]
    }).render('comments');
    </script>
</div>


            </div>
        </div>
        <footer class="footer">
    Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>, Theme by <a href="https://github.com/sanonz/hexo-theme-concise" target="_blank">Concise</a>

    
    <script>
        var _hmt = _hmt || [];
        (function () {
            var hm = document.createElement("script");
            hm.src = "https://hm.baidu.com/hm.js?ff62cebe12c54c12ef10722f77d75303";
            var s = document.getElementsByTagName("script")[0];
            s.parentNode.insertBefore(hm, s);
        })();
    </script>
    
</footer>

    </main>

    <script type="text/javascript" src="https://cdn.bootcss.com/jquery/1.9.0/jquery.min.js"></script>
    <script type="text/javascript">
    $(function() {
        var nodes = {
            nav: $('#nav'),
            aside: $('#aside'),
            asideInner: $('#aside-inner'),
            navInner: $('#nav-inner')
        };

        var doing = false;
        nodes.asideInner.on('webkitAnimationEnd mozAnimationEnd oAnimationEnd oanimationend animationend', function() {
            if (nodes.aside.hasClass('mobile-open')) {
                nodes.aside.removeClass('mobile-open');
            } else {
                nodes.aside.removeClass('mobile-close panel-show');
            }
            doing = false;
        });
        $('#open-panel, #aside-mask').on('click', function() {
            if (doing) {
                return;
            }

            if (nodes.aside.hasClass('panel-show')) {
                nodes.aside.addClass('mobile-close');
            } else {
                nodes.aside.addClass('mobile-open panel-show');
            }
        });
        $('#open-menus').on('click', function() {
            nodes.navInner.slideToggle();
        });

        if (window.innerWidth <= 960) {
            setTimeout(function() {
                nodes.navInner.slideUp();
            }, 3000);
        }
    });
    </script>
    
        <script src="/js/scrollspy.min.js"></script>
        <script type="text/javascript">
        $(document.body).scrollspy({target: '#aside-inner'});

        $(window).on('resize', function() {
            var hw = $('#header').width();
            var ww = $('#wrapper').width();
            var space = ($(this).width() - hw - ww) / 2 / 2;

            var pageprev = $('#pageprev');
            var pagenext = $('#pagenext');
            var avg = (pageprev.width() + pagenext.width()) / 2

            if(space > avg) {
                var len = space - avg / 2;
                var styles = {position: 'fixed', top: '50%', marginTop: - (pageprev.width() + pagenext.width()) / 4}
                pageprev.css($.extend({left: hw + len}, styles));
                pagenext.css($.extend({right: len}, styles));
            } else {
                pageprev.removeAttr('style');
                pagenext.removeAttr('style');
            }
        }).trigger('resize');
        </script>
    

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</body>
</html>
